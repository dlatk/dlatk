

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>BERT in DLATK &mdash; DLATK 1.3.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" href="../_static/style.css" type="text/css" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=1f29e9d3"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html">
            
              <img src="../_static/dlatk_logo_square.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#recommended-install">Recommended Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#full-install">Full Install</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#setup">Setup</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../install.html#linux">Linux</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#osx-with-brew">OSX (with brew)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#install-pip">Install (pip)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#install-anaconda">Install (Anaconda)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#install-github">Install (GitHub)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#install-other-dependencies">Install Other Dependencies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../install.html#load-nltk-corpus">Load NLTK corpus</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#install-stanford-parser">Install Stanford Parser</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#install-tweet-nlp-v0-3-ark-tweet-nlp-0-3">Install Tweet NLP v0.3 (ark-tweet-nlp-0.3)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#python-modules-optional">Python Modules (optional)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#install-the-ibm-wordcloud-jar-file-optional">Install the IBM Wordcloud jar file (optional)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../install.html#mallet-optional">Mallet (optional)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#full-list-of-dependencies">Full List of Dependencies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#python">Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#other">Other</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#python-optional">Python (optional)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#other-optional">Other (optional)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#python-version-support">Python version support</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#getting-started">Getting Started</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../install.html#command-line-interface">Command Line Interface</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#mysql-configuration">MySQL Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#sample-datasets">Sample Datasets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../install.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#install-issues">Install Issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/dlatk/dlatk/">Github Repo</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/drive/10WMCmnKzwywZR7s2et5xx9CcoWBNmhLY?usp=sharing">Getting started in Colab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#getting-started">Getting started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#text-cleaning-and-transformations">Text Cleaning and Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#feature-extraction">Feature Extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#viewing-your-data-and-output">Viewing your data and output</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#prediction">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#clustering">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#lda-with-mallet">LDA with Mallet</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#data-engines">Data Engines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#other-topics">Other Topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials.html#video-tutorials">Video Tutorials</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../datasets.html">Packaged Datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../datasets.html#language-data">Language Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../datasets.html#blog-authorship-corpus">Blog Authorship Corpus</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../datasets.html#lexica">Lexica</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../datasets.html#age-and-gender-lexica">Age and Gender Lexica</a></li>
<li class="toctree-l3"><a class="reference internal" href="../datasets.html#perma-lexicon">PERMA Lexicon</a></li>
<li class="toctree-l3"><a class="reference internal" href="../datasets.html#spanish-perma-lexicon">Spanish PERMA Lexicon</a></li>
<li class="toctree-l3"><a class="reference internal" href="../datasets.html#other-lexica">Other Lexica</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../datasets.html#lda-topics">LDA Topics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../datasets.html#facebook-topics">2000 Facebook Topics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dlatkinterface_ordered.html">dlatkInterface Flags by type</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#preprocessing">Preprocessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#feature-extraction">Feature Extraction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#feature-refinement">Feature Refinement</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#language-insights">Language Insights</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#clustering">Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#prediction">Prediction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../dlatkinterface_ordered.html#regression">Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../dlatkinterface_ordered.html#classification">Classification</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../dlatkinterface_ordered.html#visualization">Visualization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../papers.html">Papers Utilizing DLATK</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../papers.html#dlatk-paper">DLATK Paper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../papers.html#peer-reviewed-publications">Peer Reviewed Publications</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id1">2020</a></li>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id2">2019</a></li>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id3">2018</a></li>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id4">2017</a></li>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id11">2016</a></li>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id21">2015</a></li>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id26">2014</a></li>
<li class="toctree-l3"><a class="reference internal" href="../papers.html#id32">2013</a></li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DLATK</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">BERT in DLATK</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/tut_bert.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="bert-in-dlatk">
<span id="tut-bert"></span><h1>BERT in DLATK<a class="headerlink" href="#bert-in-dlatk" title="Link to this heading"></a></h1>
<p>BERT is a contextual word embedding model. It converts words to numeric vectors that capture semantic information. Recommended reading about BERT:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://jalammar.github.io/illustrated-bert/">Illustrated BERT</a></p></li>
</ul>
<p>DLATK enables extraction of BERT features from messages. These features are stored, as you might expect, in feature tables!</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h2>
<p>You must have a <code class="docutils literal notranslate"><span class="pre">msgs</span></code> table in MySQL containing a <code class="docutils literal notranslate"><span class="pre">message</span></code> column.</p>
<p>You must also ensure that punkt is installed for NLTK. This can be accomplished with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>nltk.downloader<span class="w"> </span>punkt
</pre></div>
</div>
</section>
<section id="preparing-messages">
<h2>1. Preparing messages<a class="headerlink" href="#preparing-messages" title="Link to this heading"></a></h2>
<p>Messages need to be split using <a class="reference internal" href="../fwinterface/fwflag_add_sent_tokenized.html"><span class="doc">--add_sent_tokenized</span></a>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>--add_sent_tokenized
</pre></div>
</div>
<p>This will create the table <code class="docutils literal notranslate"><span class="pre">msgs_xxx_stoks</span></code> with a <code class="docutils literal notranslate"><span class="pre">messages</span></code> column containing arrays of messages. Remember to replace <code class="docutils literal notranslate"><span class="pre">xxx</span></code> with your initials!</p>
</section>
<section id="adding-bert-features">
<h2>2. Adding BERT features<a class="headerlink" href="#adding-bert-features" title="Link to this heading"></a></h2>
<p>The simplest way to add BERT embeddings, accepting all defaults, is with the <a class="reference internal" href="../fwinterface/fwflag_add_bert.html"><span class="doc">--bert_model</span></a> flag:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>--add_bert
</pre></div>
</div>
<p>Note that the BERT features this adds will be aggregated at several levels:</p>
<ul class="simple">
<li><p>Layers</p></li>
<li><p>Words</p></li>
<li><p>Messages</p></li>
</ul>
<p>BERT layers are aggregated to produce a single vector representation of a word. Words are aggregated to produce message-level vector representations. Messages are aggregated to produce vector representations at the level of the grouping factor (<a class="reference internal" href="../fwinterface/fwflag_c.html"><span class="doc">-g</span></a>). In the command above, this means messages are aggregated to produce user-level vector representations.</p>
<p>This command uses all BERT defaults. However, it is possible to customize BERT features in a number of ways:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../fwinterface/fwflag_bert_model.html"><span class="doc">--bert_model</span></a></p></li>
<li><p><a class="reference internal" href="../fwinterface/fwflag_bert_layers.html"><span class="doc">--bert_layers</span></a></p></li>
<li><p><a class="reference internal" href="../fwinterface/fwflag_bert_layer_aggregation.html"><span class="doc">--bert_layer_aggregation</span></a></p></li>
<li><p><span class="xref std std-doc">../fwinterface/fwflag_bert_word_aggregation</span></p></li>
</ul>
<p>In the following subsections, we discuss these flags in more detail.</p>
<section id="bert-model">
<h3>--bert_model<a class="headerlink" href="#bert-model" title="Link to this heading"></a></h3>
<p>The most important option for BERT is the choice of model using the <a class="reference internal" href="../fwinterface/fwflag_bert_model.html"><span class="doc">--bert_model</span></a> flag. Any <a class="reference external" href="https://huggingface.co/transformers/pretrained_models.html">Hugging Face pretrained models</a> may be used here. By default, BERT features are extracted using the <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code> model; you can specify other models like so:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>--add_bert<span class="w"> </span>--bert_model<span class="w"> </span>large-uncased
</pre></div>
</div>
<p>Note that the command above doesn't specify <code class="docutils literal notranslate"><span class="pre">bert-</span></code>. If you use a model named <code class="docutils literal notranslate"><span class="pre">base-*</span></code> or <code class="docutils literal notranslate"><span class="pre">large-*</span></code>, DLATK will assume you're referring to a BERT model. You can specify other models using the full name, for example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>--add_bert<span class="w"> </span>--bert_model<span class="w"> </span>cl-tohoku/bert-base-japanese
</pre></div>
</div>
</section>
<section id="bert-layers">
<h3>--bert_layers<a class="headerlink" href="#bert-layers" title="Link to this heading"></a></h3>
<p>BERT produces multiple layers of embeddings (because it is a <em>deep</em> network, so each network layer produces a layer of embeddings!). Roughly speaking, later layers embed more abstract representations of features, while earlier layers represent more concrete ones. It is typical to combine some or all of these layers in order to capture this breadth of representation:</p>
<img alt="../_images/bert-layers.png" src="../_images/bert-layers.png" />
<p>To specify which layers you want to aggregate over, use the <a class="reference internal" href="../fwinterface/fwflag_bert_layers.html"><span class="doc">--bert_layers</span></a> flag. This flag takes as arguments the indexes of each layer you want to keep. For example, we might run the following to keep the last two layers:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>--add_bert<span class="w"> </span>--bert_model<span class="w"> </span>large-uncased<span class="w"> </span>--bert_layers<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="m">11</span>
</pre></div>
</div>
<p>Notice that layers are 0-indexed, i.e., the 0th layer is the earliest one, while the 11th is the last one.</p>
</section>
<section id="aggregation">
<h3>Aggregation<a class="headerlink" href="#aggregation" title="Link to this heading"></a></h3>
<p>As discussed above, there are two levels of aggregation when adding BERT features: layer and message. These can be specified with the following flags:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../fwinterface/fwflag_bert_layer_aggregation.html"><span class="doc">--bert_layer_aggregation</span></a></p></li>
<li><p><a class="reference internal" href="../fwinterface/fwflag_bert_msg_aggregation.html"><span class="doc">--bert_msg_aggregation</span></a></p></li>
</ul>
<p>It is important when running these aggregations to remember that you're choosing a numpy method, and that it will be applied to the 0th axis (i.e., it will be applied across layers). Here's an example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>--add_bert<span class="w"> </span>--bert_model<span class="w"> </span>large-uncased<span class="w"> </span>--bert_layer_aggregation<span class="w"> </span>mean<span class="w"> </span>--bert_msg_aggregation<span class="w"> </span>max
</pre></div>
</div>
<p>It is also possible to specify multiple aggregations (though this currently does not work for layer aggregations). Aggregations will be applied in the order that you specify them.</p>
</section>
</section>
<section id="understanding-bert-feature-table-names">
<h2>3. Understanding BERT Feature Table Names<a class="headerlink" href="#understanding-bert-feature-table-names" title="Link to this heading"></a></h2>
<p>BERT feature tables have names that might look confusing, but actually reveal all the details about how the features were computed. If you don't yet understand feature table naming conventions in DLATK, please read <a class="reference internal" href="tut_feat_tables.html"><span class="doc">Understanding Feature Table Names</span></a> before continuing.</p>
<p>Let's say you have a BERT feature table called <code class="docutils literal notranslate"><span class="pre">feat$bert_ba_un_meL10co$messages_en$user_id$16to16</span></code>. Here's how to interpret the segment <code class="docutils literal notranslate"><span class="pre">bert_ba_un_meL10co</span></code>:</p>
<ol class="arabic simple">
<li><p><strong>bert</strong></p></li>
<li><p><strong>ba</strong>se_<strong>un</strong>cased</p></li>
<li><p><strong>me</strong>an aggregated messages</p></li>
<li><p><strong>L</strong>ayer 10</p></li>
<li><p><strong>con</strong>catenated layers</p></li>
</ol>
<p>Some of these may be repeated: each layer selected with <a class="reference internal" href="../fwinterface/fwflag_bert_layers.html"><span class="doc">--bert_layers</span></a>, for example, will appear in the name.</p>
</section>
<section id="using-bert-features">
<h2>4. Using BERT features<a class="headerlink" href="#using-bert-features" title="Link to this heading"></a></h2>
<p>Let's say you've generated default BERT features with the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>--add_bert
</pre></div>
</div>
<p>This will create the table <code class="docutils literal notranslate"><span class="pre">feat$bert_ba_un_meL10con$msgs_xxx$user_id$16to16</span></code> in the <code class="docutils literal notranslate"><span class="pre">dla_tutorial</span></code> database. How do you make use of these features?</p>
<p>The answer is, essentially, like any other feature table in DLATK! (See <a class="reference internal" href="tut_pred.html"><span class="doc">Intro Prediction / Classification / Predictive Lexica</span></a> if you don't know how to use feature tables.) For example, let's say you want to predict age from the <code class="docutils literal notranslate"><span class="pre">blog_outcomes</span></code> table in the <code class="docutils literal notranslate"><span class="pre">dla_tutorial</span></code> database (just like in <a class="reference internal" href="tut_pred.html"><span class="doc">Intro Prediction / Classification / Predictive Lexica</span></a>). This would look like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>dlatkInterface.py<span class="w"> </span>-d<span class="w"> </span>dla_tutorial<span class="w"> </span>-t<span class="w"> </span>msgs_xxx<span class="w"> </span>-c<span class="w"> </span>user_id<span class="w"> </span>-f<span class="w"> </span><span class="s1">&#39;feat$bert_ba_un_meL10con$msgs_xxx$user_id$16to16&#39;</span><span class="w"> </span>--outcome_table<span class="w"> </span>blog_outcomes<span class="w"> </span>--group_freq_thresh<span class="w"> </span><span class="m">500</span><span class="w"> </span>--outcomes<span class="w"> </span>age<span class="w"> </span>--output_name<span class="w"> </span>xxx_age_output<span class="w"> </span>--nfold_test_regression<span class="w"> </span>--model<span class="w"> </span>ridgecv<span class="w"> </span>--folds<span class="w"> </span><span class="m">10</span>
</pre></div>
</div>
<p>This will run a ridge regression model through 10-fold cross validation, predicting age, and using BERT embeddings as features in the model. You should see lots of output, ending with something like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>TEST<span class="w"> </span>COMPLETE<span class="o">]</span>

<span class="o">{</span><span class="s1">&#39;age&#39;</span>:<span class="w"> </span><span class="o">{()</span>:<span class="w"> </span><span class="o">{</span><span class="m">1</span>:<span class="w"> </span><span class="o">{</span><span class="s1">&#39;N&#39;</span>:<span class="w"> </span><span class="m">978</span>,
<span class="w">          </span><span class="s1">&#39;R&#39;</span>:<span class="w"> </span><span class="m">0</span>.6618386965904822,
<span class="w">          </span><span class="s1">&#39;R2&#39;</span>:<span class="w"> </span><span class="m">0</span>.43803046030458836,
<span class="w">          </span><span class="s1">&#39;R2_folds&#39;</span>:<span class="w"> </span><span class="m">0</span>.4233643081827411,
<span class="w">          </span><span class="s1">&#39;mae&#39;</span>:<span class="w"> </span><span class="m">4</span>.619034298134363,
<span class="w">          </span><span class="s1">&#39;mae_folds&#39;</span>:<span class="w"> </span><span class="m">4</span>.616423448270654,
<span class="w">          </span><span class="s1">&#39;mse&#39;</span>:<span class="w"> </span><span class="m">38</span>.854440157161314,
<span class="w">          </span><span class="s1">&#39;mse_folds&#39;</span>:<span class="w"> </span><span class="m">38</span>.820150245521624,
<span class="w">          </span><span class="s1">&#39;num_features&#39;</span>:<span class="w"> </span><span class="m">768</span>,
<span class="w">          </span><span class="s1">&#39;r&#39;</span>:<span class="w"> </span><span class="m">0</span>.6621098985283438,
<span class="w">          </span><span class="s1">&#39;r_folds&#39;</span>:<span class="w"> </span><span class="m">0</span>.6694097453791333,
<span class="w">          </span><span class="s1">&#39;r_p&#39;</span>:<span class="w"> </span><span class="m">2</span>.0416764169778933e-124,
<span class="w">          </span><span class="s1">&#39;r_p_folds&#39;</span>:<span class="w"> </span><span class="m">1</span>.783371417616072e-11,
<span class="w">          </span><span class="s1">&#39;rho&#39;</span>:<span class="w"> </span><span class="m">0</span>.7107187086007063,
<span class="w">          </span><span class="s1">&#39;rho_p&#39;</span>:<span class="w"> </span><span class="m">2</span>.9543434601896134e-151,
<span class="w">          </span><span class="s1">&#39;se_R2_folds&#39;</span>:<span class="w"> </span><span class="m">0</span>.02206729102836306,
<span class="w">          </span><span class="s1">&#39;se_mae_folds&#39;</span>:<span class="w"> </span><span class="m">0</span>.11535169076175808,
<span class="w">          </span><span class="s1">&#39;se_mse_folds&#39;</span>:<span class="w"> </span><span class="m">1</span>.859884969097588,
<span class="w">          </span><span class="s1">&#39;se_r_folds&#39;</span>:<span class="w"> </span><span class="m">0</span>.01581679229802067,
<span class="w">          </span><span class="s1">&#39;se_r_p_folds&#39;</span>:<span class="w"> </span><span class="m">9</span>.484973024855944e-12,
<span class="w">          </span><span class="s1">&#39;se_train_mean_mae_folds&#39;</span>:<span class="w"> </span><span class="m">0</span>.20067386714575627,
<span class="w">          </span><span class="s1">&#39;test_size&#39;</span>:<span class="w"> </span><span class="m">105</span>,
<span class="w">          </span><span class="s1">&#39;train_mean_mae&#39;</span>:<span class="w"> </span><span class="m">4</span>.335374453708489,
<span class="w">          </span><span class="s1">&#39;train_mean_mae_folds&#39;</span>:<span class="w"> </span><span class="m">6</span>.464315148935017,
<span class="w">          </span><span class="s1">&#39;train_size&#39;</span>:<span class="w"> </span><span class="m">873</span>,
<span class="w">          </span><span class="s1">&#39;{modelFS_desc}&#39;</span>:<span class="w"> </span><span class="s1">&#39;None&#39;</span>,
<span class="w">          </span><span class="s1">&#39;{model_desc}&#39;</span>:<span class="w"> </span><span class="s1">&#39;RidgeCV(alphas=array([1.e+03, 1.e-01, &#39;</span>
<span class="w">                          </span><span class="s1">&#39;1.e+00, 1.e+01, 1.e+02, 1.e+04, 1.e+05]),   &#39;</span>
<span class="w">                          </span><span class="s1">&#39;cv=None, fit_intercept=True, gcv_mode=None, &#39;</span>
<span class="w">                          </span><span class="s1">&#39;normalize=False,   scoring=None, &#39;</span>
<span class="w">                          </span><span class="s1">&#39;store_cv_values=False)&#39;</span><span class="o">}}}}</span>
</pre></div>
</div>
<p>Comparing these results against those from <a class="reference internal" href="tut_pred.html"><span class="doc">Intro Prediction / Classification / Predictive Lexica</span></a>, we can see that BERT features get a Pearson r of 0.6621, outperforming LDA topics + unigrams, which get an r of 0.6496.</p>
<p>There's a natural question we've glossed over here: what exactly do the BERT features look like? We can check the contents of the feature table in MySQL:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mysql&gt;<span class="w"> </span>SELECT<span class="w"> </span>*<span class="w"> </span>FROM<span class="w"> </span>feat<span class="nv">$bert_ba_un_meL10con$msgs_xxx$user_id$16</span>to16<span class="w"> </span>LIMIT<span class="w"> </span><span class="m">10</span><span class="p">;</span>
+----+----------+------+-----------------------+-----------------------+
<span class="p">|</span><span class="w"> </span>id<span class="w"> </span><span class="p">|</span><span class="w"> </span>group_id<span class="w"> </span><span class="p">|</span><span class="w"> </span>feat<span class="w"> </span><span class="p">|</span><span class="w"> </span>value<span class="w">                 </span><span class="p">|</span><span class="w"> </span>group_norm<span class="w">            </span><span class="p">|</span>
+----+----------+------+-----------------------+-----------------------+
<span class="p">|</span><span class="w">  </span><span class="m">1</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>0me<span class="w">  </span><span class="p">|</span><span class="w">  </span>-0.20481809973716736<span class="w"> </span><span class="p">|</span><span class="w">  </span>-0.20481809973716736<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">2</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>1me<span class="w">  </span><span class="p">|</span><span class="w">  </span>-0.48483654856681824<span class="w"> </span><span class="p">|</span><span class="w">  </span>-0.48483654856681824<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">3</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>2me<span class="w">  </span><span class="p">|</span><span class="w">    </span><span class="m">1</span>.1650058031082153<span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">1</span>.1650058031082153<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">4</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>3me<span class="w">  </span><span class="p">|</span><span class="w">   </span>-0.5072966814041138<span class="w"> </span><span class="p">|</span><span class="w">   </span>-0.5072966814041138<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">5</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>4me<span class="w">  </span><span class="p">|</span><span class="w">   </span><span class="m">0</span>.40456074476242065<span class="w"> </span><span class="p">|</span><span class="w">   </span><span class="m">0</span>.40456074476242065<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">6</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>5me<span class="w">  </span><span class="p">|</span><span class="w">   </span>-0.6585525274276733<span class="w"> </span><span class="p">|</span><span class="w">   </span>-0.6585525274276733<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">7</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>6me<span class="w">  </span><span class="p">|</span><span class="w"> </span>-0.019926181063055992<span class="w"> </span><span class="p">|</span><span class="w"> </span>-0.019926181063055992<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">8</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>7me<span class="w">  </span><span class="p">|</span><span class="w">    </span><span class="m">0</span>.2585161030292511<span class="w"> </span><span class="p">|</span><span class="w">    </span><span class="m">0</span>.2585161030292511<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w">  </span><span class="m">9</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>8me<span class="w">  </span><span class="p">|</span><span class="w">   </span>-0.2901904881000519<span class="w"> </span><span class="p">|</span><span class="w">   </span>-0.2901904881000519<span class="w"> </span><span class="p">|</span>
<span class="p">|</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="p">|</span><span class="w">      </span><span class="m">666</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>9me<span class="w">  </span><span class="p">|</span><span class="w">   </span>-0.2664993405342102<span class="w"> </span><span class="p">|</span><span class="w">   </span>-0.2664993405342102<span class="w"> </span><span class="p">|</span>
+----+----------+------+-----------------------+-----------------------+
</pre></div>
</div>
<p>The names of the <code class="docutils literal notranslate"><span class="pre">feat</span></code> column may seem a bit opaque at first, but they are simple to interpret: the number indicates the index of the dimension in the BERT embedding vector, while the <code class="docutils literal notranslate"><span class="pre">me</span></code> indicates that the message embeddings were aggregated using the mean. If you have specified multiple message aggregations, these will appear as separate features. Since BERT produces vectors of length 768, this means each <code class="docutils literal notranslate"><span class="pre">group_id</span></code> will have <code class="docutils literal notranslate"><span class="pre">768</span> <span class="pre">*</span> <span class="pre">[number</span> <span class="pre">of</span> <span class="pre">message</span> <span class="pre">aggregations]</span></code> features. Each dimension of the aggregated BERT embedding vector then serves as a distinct feature in the predictive model.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, H. Andrew Schwartz and Salvatore Giorgi.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>